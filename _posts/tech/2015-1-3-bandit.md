---
layout: post
title: Bandits-一种简单而强大的在线学习算法
category: tech 
description: 介绍了Bandit在线学习算法，以及它的应用。
published: true

---

假设我有5个硬币，都是不均匀的。我们玩一个游戏，每次你可以选择其中一枚硬币掷出，掷硬币的次数有限（比如10000次），每次掷出正面，你将得到一百块奖励。显然，如果要拿到最多的利益，你要做的就是尽快找出“正面概率最大”的硬币，然后就拿它赚钱了。

这个问题看起来很数学化，其实它在我们的生活中经常遇见。比如我们现在有很多在线场景，遇到一个相同的问题：一个平台这么多信息，该展示什么给用户，才能有最好的收益（比如点击率）？

Google作为最大的搜索广告公司，在用户搜索时该展示什么广告；Facebook作为社交平台，当用户好友过载的时候，该怎么组织好友的说说（把你最感兴趣的放前面）；Taobao有海量的商品池子，该如何捞取用户最容易剁手的商品展示出来？

一切通过数据收集而得到的概率预估任务，都能通过Bandit系列算法来完成。

Bandit算法的创造其实来源于人类的经验，或说事人类可供传递的智慧。这个算法框架包含两个部分，一是探索未知（explore），二是利用已知（exploit），将机会分开，一部分只作探索（不考虑曾经的经验），一部分只做采集（利用已知的最好策略）。

俗话说，搜索和推荐不分家，而排序作为两者极其重要的组成部分，也有其美妙之处。Bandit算法就是解决在线排序问题的一把好刀，甚至它的后代也绽放着异常的光芒。

## How Bandit

首先来看看Bandit的概率原理，我们希望知道每一个硬币“正面”的概率 $p_i$ 。事实上我们能观察到的，只是这个硬币正面的频率

$$ \mu_i = \frac{正面次数}{全部尝试次数}$$

怎么利用起观察到的频率，来最好地预估真实的概率呢？下面介绍4种策略，分别是随机（Random）、简单观察（Naive）、ε-贪心法、置信上限法。

### Random

每次随机选择一枚硬币进行投掷。如果不能胜过这个策略，就不必玩了。

### Naive

先给每个硬币一定次数的尝试，比如每个硬币掷10次，根据每个硬币正面朝上的次数，选择正面频率最高的那个硬币，作为最佳策略。这也是大多人能想到的方法。

但是这个策略有几个明显问题：

1. 10次尝试真的靠谱吗？最差的硬币也有可能在这10次内有高于最好硬币的正面次数。
2. 假设你选到的这个硬币在投掷次数多了后发生了问题（比如掉屑），改变了其属性，那不是没有改变的机会了？（这是对变量的考虑）
3. 就算你给一个硬币10次机会，如果硬币真的很多，比如K>100，给每个硬币10次机会会不会太浪费了呢？等所有硬币都尝试过，再回来“赚钱”，花儿都谢了！


### ε-Greedy

有了前两个垫背，可以开始让Bandit登场了。ε-Greedy就是一种很机智的Bandit算法：它让每次机会以ε的概率去“探索”，1-ε的概率来“开发”。也即，如果一次机会落入ε中，则随机选择一个硬币来投掷，否则就选择先前探索到正面概率最大的硬币。这就是在线算法(Online Algorithm)，并不是说互联网意义上的在线，而是指它的模型参数在不断随着时间进行更新，并慢慢趋于最优。这个策略有两个好处：

1. 它能够应对变化，如果硬币“变质”了，它也能及时改变策略。
2. ε-Greedy机制让玩的过程更有趣，有时“探索”，有时“赚钱”。

在此基础上，又能引申出很多值得研究的问题，比如ε应该如何设定呢？它应不应该随着时间而变？因为随着探索次数的增多，好的选择自然浮现得比较明显了。ε大则使得模型有更大的灵活性（能更快的探索到未知，适应变化），ε小则会有更好的稳定性（有更多机会去“开发”）。

### UCB

在统计学中，对于一个未知量的估计，总能找到一种量化其置信度的方法。最普遍的分布正态分布（或曰高斯分布）N(E,delta)，其中的E就是估计量的期望，而delta则表示其不确定性（delta越大则表示越不可信）。比如你掷一个标准的6面色子，它的平均值是3.5，而如果你只掷一次，比如说到2，那你对平均值的估计只能是2，但是这个置信度应该很低，我们可以知道，这个色子的预估平均值是2，而以95%的置信区间在[1.4,5.2]。

UCB（Upper Confidence Bound - 置信上限）就是以均值的置信上限为来代表它的预估值：

$$ \widehat{\mu_i } = \widehat{\mu_i} + 2\sqrt{\frac{1}{n_i}} $$

上面是一个例子，其中\mu_i是对期望的预估，n_i是尝试次数，可以看到对i的尝试越多，其预估值与置信上限的差值就越小。也就是越有置信度。

这个策略的好处是，能让没有机会尝试的硬币得到更多尝试的机会，是骡子是马拉出来溜溜！将整个探索+开发的过程融合到一个公式里面，很完美！

### 模拟结果

将这几个策略做一下模拟，取K=5个硬币，每轮10000次投掷机会，跑100轮取平均。得到结果如下：

1. 随机：每次随机取一枚硬币投掷
2. 简单观察：先给每个硬币100次机会，然后以正面概率最大的硬币为策略。
3. ε-Greedy：取ε=0.01，进行探索，1-ε进行开发。
4. UCB1：以（1 - 1/t）的上限进行探索
5. UCB-95%：取95%的置信区间进行探索

![bandit_simulation](http://findshine.qiniudn.com/figure.png)

上图以累积后悔（Cumulative Expected Regret）来作为评估指标，横坐标是投掷次序，纵坐标是累积后悔（取对数）。后悔最小的算法最好。Regret定义如下：

$$ R_T=\sum_{i=1}^{T}(w_{opt}-w_{B(i)}) $$

可以看出，随机的效果最烂，Naive算法在前5*100轮跟随机效果一样烂（因为在收集数据，没有开始利用）。ε-Greedy的收敛效果好，但因为有那ε的浪费，到最后还是跟Naive一样浪费了很多机会。UCB的表现最好，收敛快、花费小！

这里只是模拟了固定概率下这些算法的表现，如果预估量（正面概率）是一个会变的量，这些算法的表现会重新洗牌吗？后续可以探索下！

## Bandit application

说了这么多掷硬币，这个算法在真实世界有什么大展身手的地方呢？小列一些：

* 在线排序（Online Ranking）
	* CTR预估
* Stock Option
	* 选择最好的股票进行投资
* A/B test
	* 快速选择好的AB版本，快速淘汰差的

附1：参考链接

* [Bandits for Recommendation Systems](http://engineering.richrelevance.com/bandits-recommendation-systems/)
* [Bayesian Methods for Hackers - Chapter6](http://nbviewer.ipython.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter6_Priorities/Priors.ipynb)


附2：模拟代码

    import numpy as np
    from matplotlib import pylab as plt
    #from mpltools import style # uncomment for prettier plots
    #style.use(['ggplot'])

    # generate all bernoulli rewards ahead of time
    def generate_bernoulli_bandit_data(num_samples,K):
        CTRs_that_generated_data = np.tile(np.random.rand(K),(num_samples,1))
        true_rewards = np.random.rand(num_samples,K) < CTRs_that_generated_data
        return true_rewards,CTRs_that_generated_data

    # totally random
    def random(observed_data):
        return np.random.randint(0,len(observed_data))

    # the naive algorithm
    def naive(observed_data,number_to_explore=100):
        totals = observed_data.sum(1) # totals
        if np.any(totals < number_to_explore): # if have been explored less than specified
            least_explored = np.argmin(totals) # return the one least explored
            return least_explored
        else: # return the best mean forever
            successes = observed_data[:,0] # successes
            estimated_means = successes/totals # the current means
            best_mean = np.argmax(estimated_means) # the best mean
            return best_mean

    # the epsilon greedy algorithm
    def epsilon_greedy(observed_data,epsilon=0.01):
        totals = observed_data.sum(1) # totals
        successes = observed_data[:,0] # successes
        estimated_means = successes/totals # the current means
        best_mean = np.argmax(estimated_means) # the best mean
        be_exporatory = np.random.rand() < epsilon # should we explore?
        if be_exporatory: # totally random, excluding the best_mean
            other_choice = np.random.randint(0,len(observed_data))
            while other_choice == best_mean:
                other_choice = np.random.randint(0,len(observed_data))
            return other_choice
        else: # take the best mean
            return best_mean

    # the UCB algorithm using 
    # (1 - 1/t) confidence interval using Chernoff-Hoeffding bound)
    # for details of this particular confidence bound, see the UCB1-TUNED section, slide 18, of: 
    # http://lane.compbio.cmu.edu/courses/slides_ucb.pdf
    def UCB(observed_data):
        t = float(observed_data.sum()) # total number of rounds so far
        totals = observed_data.sum(1)
        successes = observed_data[:,0]
        estimated_means = successes/totals # sample mean
        estimated_variances = estimated_means - estimated_means**2    
        UCB = estimated_means + np.sqrt( np.minimum( estimated_variances + np.sqrt(2*np.log(t)/totals), 0.25 ) * np.log(t)/totals )
        return np.argmax(UCB)

    # the UCB algorithm - using fixed 95% confidence intervals
    # see slide 8 for details: 
    # http://dept.stat.lsa.umich.edu/~kshedden/Courses/Stat485/Notes/binomial_confidence_intervals.pdf
    def UCB_normal(observed_data):
        totals = observed_data.sum(1) # totals
        successes = observed_data[:,0] # successes
        estimated_means = successes/totals # sample mean
        estimated_variances = estimated_means - estimated_means**2
        UCB = estimated_means + 1.96*np.sqrt(estimated_variances/totals)
        return np.argmax(UCB)

    # Thompson Sampling
    # http://www.economics.uci.edu/~ivan/asmb.874.pdf
    # http://camdp.com/blogs/multi-armed-bandits
    def thompson_sampling(observed_data):
        return np.argmax( np.random.beta(observed_data[:,0], observed_data[:,1]) )

    # the bandit algorithm
    def run_bandit_alg(true_rewards,CTRs_that_generated_data,choice_func):
        num_samples,K = true_rewards.shape
        # seed the estimated params
        prior_a = 1. # aka successes
        prior_b = 1. # aka failures
        observed_data = np.zeros((K,2))
        observed_data[:,0] += prior_a # allocating the initial conditions
        observed_data[:,1] += prior_b
        regret = np.zeros(num_samples)

        for i in range(0,num_samples):
            # pulling a lever & updating observed_data
            this_choice = choice_func(observed_data)

            # update parameters
            if true_rewards[i,this_choice] == 1:
                update_ind = 0
            else:
                update_ind = 1
                
            observed_data[this_choice,update_ind] += 1
            print choice_func.__name__, "round:" ,i, "choice:", this_choice, true_rewards[i, this_choice], observed_data[this_choice, :]
            
            # updated expected regret
            regret[i] = np.max(CTRs_that_generated_data[i,:]) - CTRs_that_generated_data[i,this_choice]

        cum_regret = np.cumsum(regret)

        return cum_regret
        
    # define number of samples and number of choices
    num_samples = 10000
    K = 5
    number_experiments = 100

    regret_accumulator = np.zeros((num_samples,5))
    for i in range(number_experiments):
        print "Running experiment:", i+1
        true_rewards,CTRs_that_generated_data = generate_bernoulli_bandit_data(num_samples,K)
        regret_accumulator[:,0] += run_bandit_alg(true_rewards,CTRs_that_generated_data,random)
        regret_accumulator[:,1] += run_bandit_alg(true_rewards,CTRs_that_generated_data,naive)
        regret_accumulator[:,2] += run_bandit_alg(true_rewards,CTRs_that_generated_data,epsilon_greedy)
        regret_accumulator[:,3] += run_bandit_alg(true_rewards,CTRs_that_generated_data,UCB)
        regret_accumulator[:,4] += run_bandit_alg(true_rewards,CTRs_that_generated_data,UCB_normal)
        
    plt.semilogy(regret_accumulator/number_experiments)
    plt.title('Simulated Bandit Performance for K = 5')
    plt.ylabel('Cumulative Expected Regret')
    plt.xlabel('Round Index')
    plt.legend(('Random','Naive','Epsilon-Greedy','(1 - 1/t) UCB','95% UCB'),loc='lower right')
    plt.show()

<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
